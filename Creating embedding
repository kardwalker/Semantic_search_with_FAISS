from transformers import AutoTokenizer, AutoModel

import torch

def get_embedding(text_list : str ,checkpoint : str = "sentence-transformers/multi-qa-mpnet-base-dot-v1" ):
  model = AutoModel.from_pretrained(checkpoint)
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  encoded_input = tokenizer(text_list , padding=True , truncation=True, return_tensors= "pt")
  encoded_input = {k : v.to(device) for k, v in encoded_input.items()}

  with torch.no_grad():
    model_output = model(**encoded_input)
  embedding = model_output.last_hidden_state[: , 0 ,:]
  return embedding , embedding.shape
